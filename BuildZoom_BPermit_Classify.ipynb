{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** This script runs the previously trained permit subtype Naive Bayes models to classify permits across the whole Buildzoom Building permit database. ***\n",
    "\n",
    "*** Additionally, it also uploads the classification to Buildzoom's database.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General declarations, including used libraries, support and plotting packages.\n",
    "Python version 2.7+ must be installed along with the following packages:\n",
    "os (built-in), numpy, pandas, nltk (Natural Language Processing Toolkit), codecs, sklearn (statistical learning package), mpld3, MySQLdb, pylab, csv, string, matplotlib (plotting), seaborn (plotting), pickle (to save clusters and other data objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3\n",
    "import MySQLdb\n",
    "import pandas.io.sql as sql\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as py\n",
    "import csv\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from pystruct.learners import NSlackSSVM\n",
    "from pystruct.models import MultiLabelClf\n",
    "from collections import defaultdict\n",
    "\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "stopwords=set(unicode(\"none system john american james hbcode thomas david michael robert subtype richard permit building none america rochester william needs brother jessica give like send estimate im youre chat details regarding hi available email call please interested contact looking project need job phone work so some somebody somehow someone something sometime sometimes somewhat somewhere soon sorry specified specify specifying still sub such sup sure t's take taken tell tends th than thank thanks thanx that that's thats the their theirs them themselves then thence there there's thereafter thereby therefore therein theres thereupon these they they'd they'll they're they've think third this thorough thoroughly those though three through throughout thru thus to together too took toward towards tried tries truly try trying twice two un under unfortunately unless unlikely until unto up upon us use used useful uses using usually value various very via viz vs want wants was wasn't way we we'd we'll we're we've welcome well went were weren't what what's whatever when whence whenever where where's whereafter whereas whereby wherein whereupon wherever whether which while whither who who's whoever whole whom whose why will willing wish with within without won't wonder would wouldn't yes yet you you'd you'll you're you've your yours yourself yourselves zero a about above after again against all am an and any are aren't as at be because been before being below between both but by can can't cannot could couldn't did didn't do does doesn't doing don't down during each few for from further had hadn't has hasn't have haven't having he he'd he'll he's her here here's hers herself him himself his how how's i i'd i'll i'm i've if in into is isn't it it's its itself let's me more most mustn't my myself no nor not of off on one once only or other ought our ours ourselves out over own same shan't she she'd she'll she's should shouldn't so some such than that that's the their theirs them themselves then there there's these they they'd they'll they're they've this those through to too under until up very was wasn't we we'd we'll we're we've were weren't what what's when when's where where's which while who who's whom why why's with won't would wouldn't you you'd you'll you're you've your yours yourself yourselves\").split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General functions for reading data, doing cluster to keyword mapping and text pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to read words associated to permit subtypes\n",
    "# Input: file_name\n",
    "# Output: list of identifier for each trade/permit type (label), list of 'synonym' words for each permit subtype. \n",
    "def read_words(input_file):\n",
    "    label=[]\n",
    "    words=[]\n",
    "    with open(input_file, 'rU') as f:\n",
    "        reader = csv.reader(f,delimiter='\\n')\n",
    "        # For each row in the file, read the job_values and the associated strings. \n",
    "        for row in reader:\n",
    "            row=row[0].split(',')\n",
    "            label.append(row.pop(0))\n",
    "            temp=[]\n",
    "            for x in row:\n",
    "                x=x.lower()\n",
    "                x=x.replace('\"','')\n",
    "                x=x.replace(' ','')\n",
    "                if x != '':\n",
    "                    temp.append(x)\n",
    "            words.append(temp)\n",
    "    return label,words\n",
    "\n",
    "# Function to join text for a given list. \n",
    "# Inputs: list with strings\n",
    "def join_txt(x):\n",
    "    content = ' '.join(filter(None,x))\n",
    "    return content\n",
    "\n",
    "# Text pre-processing function, that also applies the Snowball stemmer to each word\n",
    "# This function's only difference compared to content2tokens is the use of the stemmer. \n",
    "# Filter out spaces, punctuation, unnecessary whitespaces. \n",
    "# Any stop-word is also removed\n",
    "# Additionally, this function also removes any word smaller than 4 characters\n",
    "# Inputs: list with string for each building permit\n",
    "def content2stems(s):\n",
    "    # Initialize Snowball stemmer\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    # Transform all letters to lower-case\n",
    "    content = s.lower()\n",
    "    # Remove periods\n",
    "    content = content.replace('.', ' ')    \n",
    "    # Split words\n",
    "    content = ' '.join(content.split())\n",
    "    # Remove punctuation\n",
    "    for c in content:\n",
    "        if c in string.punctuation or c.isdigit():\n",
    "            content=content.replace(c,\" \")\n",
    "    \n",
    "    # Split words into a list\n",
    "    content = content.split() \n",
    "    # Filter words from the stoplist\n",
    "    content = filter(lambda w: w not in stopwords, content)  \n",
    "    # Filter words smaller than 3 characters\n",
    "    content = filter(lambda w: len(w) > 3, content)\n",
    "    # Apply Snowball stemmer to words\n",
    "    content = [stemmer.stem(t) for t in content]    \n",
    "    return content\n",
    "\n",
    "# Function to fetch permits in database, given a minimum id index and the chunksize to return\n",
    "# Input: start id, and chunk size to fetch (i.e. the number of permits to grab query for)\n",
    "# Output: flag indicating if anything was fetched, permit text (as a list) and permit_ids\n",
    "def fetch_permits(start,chunk_size):\n",
    "    flag=False\n",
    "    # Open database connection\n",
    "    connect = MySQLdb.connect(host,username,passwr,db)\n",
    "\n",
    "    # Fetch a subset of the permit data\n",
    "    query='SELECT id, description, type, subtype, building_type, business_name from building_permits '\n",
    "    query=query + 'WHERE id > ' + str(start) + ' '\n",
    "    query=query + 'ORDER BY id LIMIT 0,' + str(chunk_size)\n",
    "    permits= sql.read_sql(query, connect)\n",
    "    # Close connection to database\n",
    "    connect.close()\n",
    "    \n",
    "    # Save ids for each service request\n",
    "    permit_ids = permits.id\n",
    "    permits=permits.drop('id',1)\n",
    "    if len(permits) > 0:\n",
    "        # Merge pandas dataframe to list by row entry\n",
    "        permits=permits.values.tolist()\n",
    "        permits=map(join_txt,permits)\n",
    "        permits=map(lambda x: unicode(x,errors='ignore'),permits)\n",
    "        flag=True\n",
    "        \n",
    "    return flag, permits, permit_ids\n",
    "\n",
    "# Function to classify permits into every of the permit subtypes.\n",
    "# Input: terms(bag of words), permit text (as a list), keyword_label (identifier word for each permit subytpe)\n",
    "# Output: 0-1 matrix with the classification of each building permit into the distinct subtypes\n",
    "def classify_permits(terms,permits,keyword_label):\n",
    "    # Build tf/idf matrix\n",
    "    tfidf_vectorizer = TfidfVectorizer(vocabulary=terms, stop_words=stopwords,\n",
    "                                   use_idf=True, tokenizer=content2stems,\n",
    "                                   decode_error='ignore',ngram_range=[1,1])\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(permits)\n",
    "\n",
    "    classification=[]\n",
    "    # Run the Naive Bayes classifier for each subtype\n",
    "    for i, key in enumerate(keyword_label):\n",
    "        # Read the NB classifiers\n",
    "        cls=joblib.load('./NB_Classifiers/NB_'+str(key)+'.pkl')\n",
    "    \n",
    "        # Run the classifiers across the service requests\n",
    "        classification.append(cls.predict(tfidf_matrix))\n",
    "    \n",
    "    classification=np.array(classification)\n",
    "    classification=np.matrix.transpose(classification)\n",
    "    classification=pd.DataFrame(classification,columns=keyword_label)\n",
    "    \n",
    "    return classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read permits from sql database and build the appropriate text object and save the permit ids. Also read the labels for the subtypes and their associated keywords.\n",
    "\n",
    "Then, Open the previously save terms from the vectorization over permit corpus and then build the tf/idf for the classification.\n",
    "\n",
    "Run the Naive Bayes models to classify the building permits and finally input these into the Buildzoom database.\n",
    "\n",
    "This step is iterated so classification is done in 'chunks' of permits instead of loading the entire table at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fa67b50aecc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpermits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpermit_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfetch_permits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermit_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-713330e7d76f>\u001b[0m in \u001b[0;36mfetch_permits\u001b[0;34m(start, chunk_size)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'WHERE id > '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'ORDER BY id LIMIT 0,'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mpermits\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0;31m# Close connection to database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mconnect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dkedmun/Documents/Stanford/S_Internship/BuildZoom/vZoom/lib/python2.7/site-packages/pandas/io/sql.pyc\u001b[0m in \u001b[0;36mread_sql\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mcoerce_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m             chunksize=chunksize)\n\u001b[0m\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dkedmun/Documents/Stanford/S_Internship/BuildZoom/vZoom/lib/python2.7/site-packages/pandas/io/sql.pyc\u001b[0m in \u001b[0;36mread_query\u001b[0;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1504\u001b[0;31m         \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1505\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol_desc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol_desc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dkedmun/Documents/Stanford/S_Internship/BuildZoom/vZoom/lib/python2.7/site-packages/pandas/io/sql.pyc\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m                 \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                 \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1473\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dkedmun/Documents/Stanford/S_Internship/BuildZoom/vZoom/lib/python2.7/site-packages/MySQLdb/cursors.pyc\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, query, args)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             if m.args[0] in (\"not enough arguments for format string\",\n",
      "\u001b[0;32m/Users/dkedmun/Documents/Stanford/S_Internship/BuildZoom/vZoom/lib/python2.7/site-packages/MySQLdb/cursors.pyc\u001b[0m in \u001b[0;36m_query\u001b[0;34m(self, q)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0mrowcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrowcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dkedmun/Documents/Stanford/S_Internship/BuildZoom/vZoom/lib/python2.7/site-packages/MySQLdb/cursors.pyc\u001b[0m in \u001b[0;36m_do_query\u001b[0;34m(self, q)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_executed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrowcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dkedmun/Documents/Stanford/S_Internship/BuildZoom/vZoom/lib/python2.7/site-packages/MySQLdb/cursors.pyc\u001b[0m in \u001b[0;36m_do_get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrowcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffected_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrownumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dkedmun/Documents/Stanford/S_Internship/BuildZoom/vZoom/lib/python2.7/site-packages/MySQLdb/cursors.pyc\u001b[0m in \u001b[0;36m_get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    349\u001b[0m     query, or using CursorUseResultMixIn instead.\"\"\"\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Read building permit subtypes keywords and stem them.\n",
    "keyword_label,keywords=read_words('taxonomy_cluster_keywords.csv')\n",
    "keyword_label=map(lambda x: x.lower(),keyword_label)\n",
    "\n",
    "# Open lexicon terms want to use them from a previously saved file.\n",
    "with open('./NB_Classifiers/permit_terms.pickle') as f:\n",
    "    terms= pickle.load(f)\n",
    "\n",
    "terms=terms[0]\n",
    "\n",
    "# Host, username, password, database to connect\n",
    "host='db.dev.buildzoom.com'\n",
    "username=\n",
    "passwr=\n",
    "db='bzdb'\n",
    "\n",
    "# Initialization\n",
    "start=21649160\n",
    "chunk_size=30000\n",
    "# Run until the classification has transversed the whole sql table\n",
    "flag=True\n",
    "while (flag):\n",
    "    flag,permits,permit_ids=fetch_permits(start,chunk_size)\n",
    "    if (flag):\n",
    "        start=max(permit_ids)\n",
    "        classification=classify_permits(terms,permits,keyword_label)\n",
    "        # Add service request ids to the pandas dataframe\n",
    "        classification.insert(0,'permit_id',permit_ids)\n",
    "        # Host, username, password, database to connect\n",
    "        host='db.dev.buildzoom.com'\n",
    "        username='new'\n",
    "        passwr='bzdb123zoom'\n",
    "        db='bzdb'\n",
    "        # Open connection\n",
    "        connect = MySQLdb.connect(host,username,passwr,db)\n",
    "        # Write the table\n",
    "        classification.to_sql(con=connect, name='building_permits_classification', if_exists='append', \n",
    "                              flavor='mysql', index=False)\n",
    "        connect.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
