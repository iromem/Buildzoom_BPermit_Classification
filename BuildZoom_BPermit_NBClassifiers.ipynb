{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** This script constructs a Naive Bayes classifier for each permit type using the permit subtype taxonomy derived from the prior unsupervised learning step and labeling each training sample according to these. ***\n",
    "\n",
    "Briefly, this impllies performing the following steps:\n",
    "\n",
    "1) Read the taxonomy of permit subtypes and their associated 'synonym' words. \n",
    "\n",
    "2) Determine the subtypes for each building permit by on the taxonomy and their keyword (i.e. map the class labels of each permit to be used in the Naive Bayes step). This proceduces the y_train class labels. Note that this mapping is not 1-1 and one permit amy belong to several subtypes\n",
    "\n",
    "3) Vecotrize the building permits and construct a tf/idf matrix from the building permit training set. This produces the x_train features\n",
    "\n",
    "4) Train a Naive Bayes classifier on the  (we assume a multinomial distribution for the words) for each permit subtype. We use x_train set and y_train for the training labels. This implies we will have a number of classifiers equal to the number of subtypes.\n",
    "\n",
    "5) Save each of the permit type classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General declarations, including used libraries, support and plotting packages.\n",
    "Python version 2.7+ must be installed along with the following packages:\n",
    "os (built-in), numpy, pandas, nltk (Natural Language Processing Toolkit), codecs, sklearn (statistical learning package), mpld3, MySQLdb, pylab, csv, string, matplotlib (plotting), wordcloud (only if word clouds plotting is desired), seaborn (plotting), pickle (to save clusters and other data objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3\n",
    "import MySQLdb\n",
    "import pandas.io.sql as sql\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as py\n",
    "import csv\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from pystruct.learners import NSlackSSVM\n",
    "from pystruct.models import MultiLabelClf\n",
    "from collections import defaultdict\n",
    "\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "stopwords=set(unicode(\"john american james hbcode thomas david michael robert subtype richard permit building none america rochester william needs brother jessica give like send estimate im youre chat details regarding hi available email call please interested contact looking project need job phone work so some somebody somehow someone something sometime sometimes somewhat somewhere soon sorry specified specify specifying still sub such sup sure t's take taken tell tends th than thank thanks thanx that that's thats the their theirs them themselves then thence there there's thereafter thereby therefore therein theres thereupon these they they'd they'll they're they've think third this thorough thoroughly those though three through throughout thru thus to together too took toward towards tried tries truly try trying twice two un under unfortunately unless unlikely until unto up upon us use used useful uses using usually value various very via viz vs want wants was wasn't way we we'd we'll we're we've welcome well went were weren't what what's whatever when whence whenever where where's whereafter whereas whereby wherein whereupon wherever whether which while whither who who's whoever whole whom whose why will willing wish with within without won't wonder would wouldn't yes yet you you'd you'll you're you've your yours yourself yourselves zero a about above after again against all am an and any are aren't as at be because been before being below between both but by can can't cannot could couldn't did didn't do does doesn't doing don't down during each few for from further had hadn't has hasn't have haven't having he he'd he'll he's her here here's hers herself him himself his how how's i i'd i'll i'm i've if in into is isn't it it's its itself let's me more most mustn't my myself no nor not of off on one once only or other ought our ours ourselves out over own same shan't she she'd she'll she's should shouldn't so some such than that that's the their theirs them themselves then there there's these they they'd they'll they're they've this those through to too under until up very was wasn't we we'd we'll we're we've were weren't what what's when when's where where's which while who who's whom why why's with won't would wouldn't you you'd you'll you're you've your yours yourself yourselves\").split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General functions for reading data, doing cluster to keyword mapping and text pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to read words associated to permit subtypes\n",
    "# Input: file_name\n",
    "# Output: list of identifier for each trade/permit type (label), list of 'synonym' words for each permit subtype. \n",
    "def read_words(input_file):\n",
    "    label=[]\n",
    "    words=[]\n",
    "    with open(input_file, 'rU') as f:\n",
    "        reader = csv.reader(f,delimiter='\\n')\n",
    "        # For each row in the file, read the job_values and the associated strings. \n",
    "        for row in reader:\n",
    "            row=row[0].split(',')\n",
    "            label.append(row.pop(0))\n",
    "            temp=[]\n",
    "            for x in row:\n",
    "                x=x.lower()\n",
    "                x=x.replace('\"','')\n",
    "                x=x.replace(' ','')\n",
    "                if x != '':\n",
    "                    temp.append(x)\n",
    "            words.append(temp)\n",
    "    return label,words\n",
    "\n",
    "# Function to read BuildZoom building permits from a saved filed.\n",
    "# Inputs: file name that has the data\n",
    "def read_permit(input_file):\n",
    "    # List that will hold the job_values (val) and the text from the data (a)\n",
    "    a=[]\n",
    "    val=[]\n",
    "    with open(input_file, 'rU') as f:\n",
    "        reader = csv.reader(f,delimiter='\\t')\n",
    "        # For each row in the file, read the job_values and the associated strings. \n",
    "        for row in reader:\n",
    "            # Ignore rows that do not have the id(from pandas) + 6 queried features (can be NULL too)\n",
    "            if len(row) == 7:\n",
    "                # Remove the pandas id (just trash from the pandas dataframe)\n",
    "                row.pop(0)\n",
    "                try: \n",
    "                    # Save the job_value for each building permit if the value is a valid integer\n",
    "                    val.append(float(row.pop(0)))\n",
    "                except:\n",
    "                    # Simply ignore and place a zero value, if not\n",
    "                    val.append(0)\n",
    "                # Save building permit text\n",
    "                a.append(row)\n",
    "    return val,a\n",
    "\n",
    "# Function to join text for a given list. \n",
    "# Inputs: list with strings\n",
    "def join_txt(x):\n",
    "    content = ' '.join(filter(None,x))\n",
    "    return content\n",
    "\n",
    "# Text pre-processing function, that also applies the Snowball stemmer to each word\n",
    "# This function's only difference compared to content2tokens is the use of the stemmer. \n",
    "# Filter out spaces, punctuation, unnecessary whitespaces. \n",
    "# Any stop-word is also removed\n",
    "# Additionally, this function also removes any word smaller than 4 characters\n",
    "# Inputs: list with string for each building permit\n",
    "def content2stems(s):\n",
    "    # Initialize Snowball stemmer\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    # Transform all letters to lower-case\n",
    "    content = s.lower()\n",
    "    # Remove periods\n",
    "    content = content.replace('.', ' ')    \n",
    "    # Split words\n",
    "    content = ' '.join(content.split())\n",
    "    # Remove punctuation\n",
    "    for c in content:\n",
    "        if c in string.punctuation or c.isdigit():\n",
    "            content=content.replace(c,\" \")\n",
    "    \n",
    "    # Split words into a list\n",
    "    content = content.split() \n",
    "    # Filter words from the stoplist\n",
    "    content = filter(lambda w: w not in stopwords, content)  \n",
    "    # Filter words smaller than 3 characters\n",
    "    content = filter(lambda w: len(w) > 3, content)\n",
    "    # Apply Snowball stemmer to words\n",
    "    content = [stemmer.stem(t) for t in content]    \n",
    "    return content\n",
    "\n",
    "# Function to label permit subtypes to individual samples using the cluster assignements\n",
    "# Input: permit raw text (permits), list of keywords associated to each subtype and label for each subtype\n",
    "# Output: matrix were rows are samples and columns are subtype assignments (1: positive, 0: negative)\n",
    "def sample_mapping(permits,keyword_map,labels):\n",
    "    sample_map=[]\n",
    "    for s in permits:\n",
    "        # Stem the building permit\n",
    "        permit_stem= content2stems(s)\n",
    "        \n",
    "        # Determine the subtypes of the stemmed permit\n",
    "        intersect_list = [filter(lambda x: x in permit_stem, sublist) for sublist in keyword_map]\n",
    "        sample_map.append([1 if len(x) > 0 else 0 for x in intersect_list])\n",
    "        \n",
    "    return pd.DataFrame(sample_map,columns=labels)\n",
    "\n",
    "# Function to stem the keywords associated to building permit subtypes\n",
    "# Input: list of building subtypes in which each list has a list of keywords associated to that subtype\n",
    "# Outputs: list of building subtypes in which each list has a list of \n",
    "def stem_keywords(keyword_list):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    keyword_stems=[]\n",
    "    # For every list of keywords associated to a building permit, stem all keywords\n",
    "    for l in keyword_list:\n",
    "        l=map(lambda x: stemmer.stem(x),l)\n",
    "        # Eliminate potential duplicates\n",
    "        l=list(set(l))\n",
    "        keyword_stems.append(l)\n",
    "    \n",
    "    return keyword_stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the keywords associated to each building permit subtype and then stem them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read building permit subtypes keywords and stem them.\n",
    "keyword_label,keywords=read_words('taxonomy_cluster_keywords.csv')\n",
    "keywords_stems=stem_keywords(keywords)\n",
    "# Turn all label subtypes to lower_case\n",
    "keyword_label=map(lambda x: x.lower(),keyword_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all building permits to be used for training, stem them and related them to the building subtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read input files and format original data\n",
    "job_values, build_permits=read_permit('./Data/BPermit_Sample_Dke.tsv')\n",
    "build_permits=map(join_txt,build_permits)\n",
    "build_permits=map(lambda x : unicode(x, errors='ignore'),build_permits)\n",
    "# Stem, return\n",
    "# Use the raw text, apply filters and tokenize. \n",
    "map_train=sample_mapping(build_permits,keywords_stems,keyword_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build tf/idf matrix for the training set data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build tf/idf matrix for the classifiers\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=300,\n",
    "                                   min_df=50, stop_words=stopwords,\n",
    "                                   use_idf=True, tokenizer=content2stems, \n",
    "                                   decode_error='ignore',ngram_range=[1,1])\n",
    "tfidf_matrix_train = tfidf_vectorizer.fit_transform(build_permits)\n",
    "tfidf_matrix_train=tfidf_matrix_train.toarray()\n",
    "terms = tfidf_vectorizer.get_feature_names() # Save the terms that remain from the vectorization\n",
    "\n",
    "# Save building permit terms used when training\n",
    "with open('./NB_Classifiers/permit_terms.pickle', 'w') as f:\n",
    "    pickle.dump([terms], f)\n",
    "\n",
    "# Also convert the permit labels (y, for the predictions) into a numpy array\n",
    "map_train=np.array(map_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a Naive Bayes classifier for each permit subtype, and save them to disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a Naive Bayes classifier for each subtype\n",
    "scores_indiv=[]\n",
    "predict_test=[]\n",
    "for i, key in enumerate(keyword_label):\n",
    "    # Grab the class labels for each specific subtype\n",
    "    sample=map_train[:,i]\n",
    "\n",
    "    # Build Naive Bayes classifier, given tfidf-matrix and class labels.\n",
    "    cls=MultinomialNB()\n",
    "    cls.fit(tfidf_matrix_train,sample)\n",
    "    \n",
    "    # Save NB model to file for future use\n",
    "    joblib.dump(cls,'./NB_Classifiers/NB_'+str(key)+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
